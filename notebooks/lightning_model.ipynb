{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Pytorch Lightning implementation\n",
    "In this notebook I will implement CNN model using Pytorch Lightning.\n",
    "This model will be more flexible, than model from `initial_experiments.ipynb`, to provide more hyperparameters for training sessions.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3adf995104ba7a5c"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pytorch_lightning as pl\n",
    "import torchmetrics as tm\n",
    "\n",
    "from torch.nn.modules.loss import _Loss\n",
    "\n",
    "\n",
    "class RetinoCNN(pl.LightningModule):\n",
    "    \"\"\"\n",
    "    A Convolutional Neural Network (CNN) implemented using PyTorch Lightning.\n",
    "    Loss function is BCELoss, optimizer is Adam.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    conv_layers : int\n",
    "        The number of convolutional layers.\n",
    "    fc_layer_sizes : tuple of int\n",
    "        The sizes of the fully connected layers.\n",
    "    input_size : torch.Size\n",
    "        The size of the input tensor.\n",
    "    out_classes : int, optional\n",
    "        The number of output classes, default is 2.\n",
    "    initial_filters : int, optional\n",
    "        The number of filters in the first convolutional layer, default is 32.\n",
    "    hl_kernel_size : int, optional\n",
    "        The kernel size for the hidden layers, default is 5.\n",
    "    activation_func : nn.Module, optional\n",
    "        The activation function to use, default is nn.ReLU.\n",
    "    max_pool_kernel : int, optional\n",
    "        The kernel size for max pooling, default is 2.\n",
    "    dropout_conv : bool, optional\n",
    "        Whether to apply dropout to the convolutional layers, default is False.\n",
    "    dropout_fc : bool, optional\n",
    "        Whether to apply dropout to the fully connected layers, default is False.\n",
    "    dropout_rate : float, optional\n",
    "        The dropout rate, default is 0.5.\n",
    "    initial_learning_rate : float, optional\n",
    "        The initial learning rate, default is 0.01.\n",
    "    metrics : dict[str, tm.Metric]|None, optional\n",
    "        The metrics to use, default is None. If None, default metrics will be used.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "            self,\n",
    "            *,\n",
    "            conv_layers: int,\n",
    "            fc_layer_sizes: tuple[int, ...],\n",
    "            input_size: torch.Size,\n",
    "            out_classes: int = 2,\n",
    "            initial_filters: int = 32,\n",
    "            hl_kernel_size: int = 5,\n",
    "            activation_func: nn.Module = nn.ReLU,\n",
    "            max_pool_kernel: int = 2,\n",
    "            dropout_conv: bool = False,\n",
    "            dropout_fc: bool = False,\n",
    "            dropout_rate: float = 0.5,\n",
    "            initial_learning_rate: float = 0.01,\n",
    "            metrics: dict[str, tm.Metric]|None = None\n",
    "    ) -> None:\n",
    "        \n",
    "        # Validate inputs before calling super().__init__()\n",
    "        self._validate_required_inputs(conv_layers, fc_layer_sizes, input_size)\n",
    "        self._validate_default_inputs(\n",
    "            out_classes,\n",
    "            initial_filters,\n",
    "            hl_kernel_size,\n",
    "            activation_func,\n",
    "            max_pool_kernel,\n",
    "            dropout_conv,\n",
    "            dropout_fc,\n",
    "            dropout_rate,\n",
    "            initial_learning_rate,\n",
    "            metrics\n",
    "        )\n",
    "        super().__init__()\n",
    "\n",
    "        # Initialize hyperparameters\n",
    "        self._initial_learning_rate = initial_learning_rate\n",
    "\n",
    "        # Initialize metrics\n",
    "        if metrics is not None:\n",
    "            self._metrics = metrics\n",
    "        else:\n",
    "            self._metrics = {\n",
    "                \"accuracy\": tm.Accuracy(task=\"binary\"),\n",
    "                \"precision\": tm.Precision(task=\"binary\"),\n",
    "                \"recall\": tm.Recall(task=\"binary\"),\n",
    "                \"f1\": tm.F1Score(task=\"binary\"),\n",
    "                \"roc_auc\": tm.AUROC(task=\"binary\"),\n",
    "            }\n",
    "\n",
    "        # Initialize convolutional layers\n",
    "        hidden_layers = []\n",
    "        in_channels = input_size[0]\n",
    "\n",
    "        for i in range(conv_layers):\n",
    "            out_channels = initial_filters * 2 ** i\n",
    "            hidden_layers.append(nn.Conv2d(in_channels, out_channels, hl_kernel_size))\n",
    "            hidden_layers.append(activation_func())\n",
    "            hidden_layers.append(nn.MaxPool2d(max_pool_kernel))\n",
    "            in_channels = out_channels\n",
    "            if dropout_conv:\n",
    "                hidden_layers.append(nn.Dropout(dropout_rate))\n",
    "        \n",
    "        self._hidden_layers = nn.Sequential(*hidden_layers)\n",
    "\n",
    "        # Initialize fully connected layers\n",
    "        in_features = self._get_conv_out_shape(input_size)\n",
    "        fc_layers = []\n",
    "        for out_features in fc_layer_sizes:\n",
    "            fc_layers.append(nn.Linear(in_features, out_features))\n",
    "            fc_layers.append(activation_func())\n",
    "            if dropout_fc:\n",
    "                fc_layers.append(nn.Dropout(dropout_rate))\n",
    "            in_features = out_features\n",
    "        \n",
    "        fc_layers.append(nn.Linear(in_features, out_classes))\n",
    "        self._fc_layers = nn.Sequential(*fc_layers)    \n",
    "        \n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass of the model.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        x : torch.Tensor\n",
    "            The input tensor\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            The output tensor\n",
    "        \"\"\"\n",
    "        x = self._hidden_layers(x)\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        x = self._fc_layers(x)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "    def training_step(self, batch: torch.Tensor, batch_idx: int) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Training step of the model.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        batch : torch.Tensor\n",
    "            The input batch\n",
    "        batch_idx : int\n",
    "            The index of the batch\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            The loss\n",
    "        \"\"\"\n",
    "        \n",
    "        self.train()\n",
    "        x, y = batch\n",
    "        x = x.to(self._device)\n",
    "        y = y.to(self._device)\n",
    "        y_pred = self(x)\n",
    "        loss = nn.BCELoss()(y_pred, y)\n",
    "        self.log(\"train_step_loss\", loss)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch: torch.Tensor, batch_idx: int) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Validation step of the model.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        batch : torch.Tensor\n",
    "            The input batch\n",
    "        batch_idx : int\n",
    "            The index of the batch\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            The loss\n",
    "        \"\"\"\n",
    "        self.eval()\n",
    "        x, y = batch\n",
    "        x = x.to(self._device)\n",
    "        y = y.to(self._device)\n",
    "        y_pred = self(x)\n",
    "        loss = nn.BCELoss()(y_pred, y)\n",
    "        self._calculate_metrics(y_pred, y)\n",
    "        \n",
    "        self.log(\"val_step_loss\", loss)        \n",
    "        return loss\n",
    "    \n",
    "    def on_validation_epoch_end(self) -> None:\n",
    "        \"\"\"\n",
    "        Log the learning rate at the end of the validation epoch.\n",
    "        \"\"\"\n",
    "        \n",
    "        self._log_metrics(\"val_ep\")\n",
    "        self._reset_metrics()\n",
    "    \n",
    "    def test_step(self, batch: torch.Tensor, batch_idx: int) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Test step of the model.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        batch : torch.Tensor\n",
    "            The input batch\n",
    "        batch_idx : int\n",
    "            The index of the batch\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            The loss\n",
    "        \"\"\"\n",
    "        x, y = batch\n",
    "        x = x.to(self._device)\n",
    "        y = y.to(self._device)\n",
    "\n",
    "        y_pred = self(x)\n",
    "        loss = nn.BCELoss()(y_pred, y)\n",
    "        \n",
    "        self._calculate_metrics(y_pred, y)        \n",
    "        self.log(\"test_step_loss\", loss)\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    def on_test_epoch_end(self) -> None:\n",
    "        \"\"\"\n",
    "        Log the metrics at the end of the test epoch.\n",
    "        \"\"\"\n",
    "        self._log_metrics(\"test_ep\")\n",
    "        self._reset_metrics()\n",
    "\n",
    "    def configure_optimizers(self) -> torch.optim.Optimizer:\n",
    "        \"\"\"\n",
    "        Configure the optimizer for the model.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        torch.optim.Optimizer\n",
    "            The optimizer\n",
    "        \"\"\"\n",
    "        return torch.optim.Adam(self.parameters(), lr=self._initial_learning_rate)\n",
    "    \n",
    "    def _get_conv_out_shape(self, input_size: torch.Size) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Calculate shape of the output of the convolutional layers.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        input_size : torch.Size\n",
    "            The size of the input tensor\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        torch.Size\n",
    "            The size of the output tensor\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            zeros = torch.zeros(*input_size, device=self.device)\n",
    "            z = self.hidden_layers(zeros)\n",
    "            z = torch.prod(torch.tensor(z.shape))\n",
    "        return z\n",
    "\n",
    "    def _validate_required_inputs(self, conv_layers, fc_layer_sizes, input_size) -> None:\n",
    "        \"\"\"Validate inputs with no default values.\"\"\"\n",
    "\n",
    "        if not isinstance(conv_layers, int) or conv_layers < 1:\n",
    "            raise ValueError(\"conv_layers must be an integer greater than 0.\")\n",
    "\n",
    "        if not isinstance(fc_layer_sizes, tuple) or not all(isinstance(i, int) for i in fc_layer_sizes):\n",
    "            raise ValueError(\"fc_layer_sizes must be a tuple of integers.\")\n",
    "\n",
    "        if not isinstance(input_size, torch.Size):\n",
    "            raise ValueError(\"input_size must be a torch.Size object.\")\n",
    "    \n",
    "    def _validate_default_inputs(self, out_classes, initial_filters, hl_kernel_size, activation_func, max_pool_kernel, dropout_conv, dropout_fc, dropout_rate, initial_learning_rate, metrics) -> None:\n",
    "        \"\"\"Validate inputs with default values.\"\"\"\n",
    "\n",
    "        if not isinstance(out_classes, int) or out_classes < 1:\n",
    "            raise ValueError(\"out_classes must be an integer greater than 0.\")\n",
    "\n",
    "        if not isinstance(initial_filters, int) or initial_filters < 1:\n",
    "            raise ValueError(\"initial_filters must be an integer greater than 0.\")\n",
    "\n",
    "        if not isinstance(hl_kernel_size, int) or hl_kernel_size < 1:\n",
    "            raise ValueError(\"hl_kernel_size must be an integer greater than 0.\")\n",
    "\n",
    "        if not isinstance(activation_func, nn.Module):\n",
    "            raise ValueError(\"activation_func must be an instance of torch.nn.Module.\")\n",
    "\n",
    "        if not isinstance(max_pool_kernel, int) or max_pool_kernel < 1:\n",
    "            raise ValueError(\"max_pool_kernel must be an integer greater than 0.\")\n",
    "\n",
    "        if not isinstance(dropout_conv, bool):\n",
    "            raise ValueError(\"dropout_conv must be a boolean.\")\n",
    "\n",
    "        if not isinstance(dropout_fc, bool):\n",
    "            raise ValueError(\"dropout_fc must be a boolean.\")\n",
    "\n",
    "        if not isinstance(dropout_rate, float) or not 0 <= dropout_rate <= 1:\n",
    "            raise ValueError(\"dropout_rate must be a float between 0 and 1.\")\n",
    "\n",
    "        if not isinstance(initial_learning_rate, float) or initial_learning_rate <= 0:\n",
    "            raise ValueError(\"initial_learning_rate must be a float greater than 0.\")\n",
    "\n",
    "        if metrics is not None and not isinstance(metrics, dict):\n",
    "            raise ValueError(\"metrics must be a dictionary of string keys and torchmetrics.Metric objects.\")\n",
    "\n",
    "    def _calculate_metrics(self, y_pred: torch.Tensor, y: torch.Tensor) -> None:\n",
    "        \"\"\"\n",
    "        Calculate the metrics.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        y_pred : torch.Tensor\n",
    "            The predicted values\n",
    "        y : torch.Tensor\n",
    "            The true values\n",
    "        \"\"\"\n",
    "        \n",
    "        for name, metric in self._metrics.items():\n",
    "            self._metrics[name] = metric(y_pred, y) \n",
    "    \n",
    "    def _log_metrics(self, prefix: str) -> None:\n",
    "        \"\"\"\n",
    "        Log the metrics.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        prefix : str\n",
    "            The prefix for the metric name\n",
    "        \"\"\"\n",
    "        for name, value in self._metrics.items():\n",
    "            self.log(f\"{prefix}_{name}\", value)\n",
    "\n",
    "    def _reset_metrics(self) -> None:\n",
    "        \"\"\"\n",
    "        Reset the metrics.\n",
    "        \"\"\"\n",
    "        for name, value in self._metrics.items():\n",
    "            self._metrics[name].reset()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-16T16:48:33.602997Z",
     "start_time": "2024-03-16T16:48:30.942624Z"
    }
   },
   "id": "910878ec04f8bc57",
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "e82dc89439d600f4"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
