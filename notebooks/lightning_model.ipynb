{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Pytorch Lightning implementation\n",
    "In this notebook I will implement CNN model using Pytorch Lightning.\n",
    "This model will be more flexible, than model from `initial_experiments.ipynb`, to provide more hyperparameters for training sessions.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3adf995104ba7a5c"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pytorch_lightning as pl\n",
    "import torchmetrics as tm\n",
    "\n",
    "\n",
    "class CNN(pl.LightningModule):\n",
    "    \"\"\"\n",
    "    A Convolutional Neural Network (CNN) implemented using PyTorch Lightning.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    conv_layers : int\n",
    "        The number of convolutional layers.\n",
    "    fc_layer_sizes : tuple of int\n",
    "        The sizes of the fully connected layers.\n",
    "    input_size : torch.Size\n",
    "        The size of the input tensor.\n",
    "    out_classes : int, optional\n",
    "        The number of output classes, default is 2.\n",
    "    initial_filters : int, optional\n",
    "        The number of filters in the first convolutional layer, default is 32.\n",
    "    hl_kernel_size : int, optional\n",
    "        The kernel size for the hidden layers, default is 5.\n",
    "    activation_func : nn.Module, optional\n",
    "        The activation function to use, default is nn.ReLU.\n",
    "    max_pool_kernel : int, optional\n",
    "        The kernel size for max pooling, default is 2.\n",
    "    dropout_conv : bool, optional\n",
    "        Whether to apply dropout to the convolutional layers, default is False.\n",
    "    dropout_fc : bool, optional\n",
    "        Whether to apply dropout to the fully connected layers, default is False.\n",
    "    dropout_rate : float, optional\n",
    "        The dropout rate, default is 0.5.\n",
    "    loss_func : nn.Module, optional\n",
    "        The loss function to use, default is nn.CrossEntropyLoss.\n",
    "    optimizer : str, optional\n",
    "        The optimizer to use, default is \"Adam\". Can be either \"Adam\" or \"SGD\".\n",
    "    initial_learning_rate : float, optional\n",
    "        The initial learning rate, default is 0.01.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    accuracy : torchmetrics.Accuracy\n",
    "        Metric to calculate accuracy.\n",
    "    precision : torchmetrics.Precision\n",
    "        Metric to calculate precision.\n",
    "    recall : torchmetrics.Recall\n",
    "        Metric to calculate recall.\n",
    "    f1 : torchmetrics.F1Score\n",
    "        Metric to calculate F1 score.\n",
    "    roc_auc : torchmetrics.AUROC\n",
    "        Metric to calculate ROC AUC score.\n",
    "    _device : torch.device\n",
    "        The device to use for computations.\n",
    "    hidden_layers : nn.Sequential\n",
    "        The sequence of hidden layers.\n",
    "    fc_layers : nn.Sequential\n",
    "        The sequence of fully connected layers.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "            self,\n",
    "            *,\n",
    "            conv_layers: int,\n",
    "            fc_layer_sizes: tuple[int, ...],\n",
    "            input_size: torch.Size,\n",
    "            out_classes: int = 2,\n",
    "            initial_filters: int = 32,\n",
    "            hl_kernel_size: int = 5,\n",
    "            activation_func: nn.Module = nn.ReLU,\n",
    "            max_pool_kernel: int = 2,\n",
    "            dropout_conv: bool = False,\n",
    "            dropout_fc: bool = False,\n",
    "            dropout_rate: float = 0.5,\n",
    "            loss_func: nn.Module = nn.CrossEntropyLoss,\n",
    "            optimizer: str = \"Adam\",\n",
    "            initial_learning_rate: float = 0.01,\n",
    "    ) -> None:\n",
    "        \n",
    "        super().__init__()\n",
    "\n",
    "        # Initialize hyperparameters\n",
    "        self.loss_func = loss_func\n",
    "        self.initial_learning_rate = initial_learning_rate\n",
    "        if optimizer == \"Adam\":\n",
    "            self.optimizer = torch.optim.Adam\n",
    "        elif optimizer == \"SGD\":\n",
    "            self.optimizer = torch.optim.SGD\n",
    "        else:\n",
    "            raise ValueError(\"Invalid optimizer. Use 'Adam' or 'SGD'.\")\n",
    "\n",
    "        # Initialize metrics\n",
    "        self.accuracy = tm.Accuracy(task=\"binary\")\n",
    "        self.precision = tm.Precision(task=\"binary\")\n",
    "        self.recall = tm.Recall(task=\"binary\")\n",
    "        self.f1 = tm.F1Score(task=\"binary\")\n",
    "        self.roc_auc = tm.AUROC(task=\"binary\")\n",
    "        \n",
    "        self._device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "        \n",
    "        \n",
    "        # Initialize convolutional layers\n",
    "        hidden_layers = []\n",
    "        in_channels = input_size[0]\n",
    "\n",
    "        for i in range(conv_layers):\n",
    "            out_channels = initial_filters * 2 ** i\n",
    "            hidden_layers.append(nn.Conv2d(in_channels, out_channels, hl_kernel_size))\n",
    "            hidden_layers.append(activation_func())\n",
    "            hidden_layers.append(nn.MaxPool2d(max_pool_kernel))\n",
    "            in_channels = out_channels\n",
    "            if dropout_conv:\n",
    "                hidden_layers.append(nn.Dropout(dropout_rate))\n",
    "        \n",
    "        self.hidden_layers = nn.Sequential(*hidden_layers)\n",
    "\n",
    "        # Initialize fully connected layers\n",
    "        conv_out_shape = self._get_conv_out_shape(input_size)\n",
    "        in_features = 8\n",
    "        fc_layers = []  \n",
    "        for out_features in fc_layer_sizes:\n",
    "            fc_layers.append(nn.Linear(conv_out_shape, out_features))\n",
    "            fc_layers.append(activation_func())\n",
    "            if dropout_fc:\n",
    "                fc_layers.append(nn.Dropout(dropout_rate))\n",
    "            in_features = out_features\n",
    "        \n",
    "        fc_layers.append(nn.Linear(in_features, out_classes))\n",
    "        self.fc_layers = nn.Sequential(*fc_layers)\n",
    "        \n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass of the model.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        x : torch.Tensor\n",
    "            The input tensor\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            The output tensor\n",
    "        \"\"\"\n",
    "        x = self.hidden_layers(x)\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        x = self.fc_layers(x)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "    def training_step(self, batch: torch.Tensor, batch_idx: int) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Training step of the model.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        batch : torch.Tensor\n",
    "            The input batch\n",
    "        batch_idx : int\n",
    "            The index of the batch\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            The loss\n",
    "        \"\"\"\n",
    "        x, y = batch\n",
    "        y_pred = self(x)\n",
    "        loss = self.loss_func(y_pred, y)\n",
    "        self.log(\"train_step_loss\", loss)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch: torch.Tensor, batch_idx: int) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Validation step of the model.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        batch : torch.Tensor\n",
    "            The input batch\n",
    "        batch_idx : int\n",
    "            The index of the batch\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            The loss\n",
    "        \"\"\"\n",
    "        x, y = batch\n",
    "        y_pred = self(x)\n",
    "        loss = self.loss_func(y_pred, y)\n",
    "        \n",
    "        self.accuracy(y_pred, y)\n",
    "        self.precision(y_pred, y)\n",
    "        self.recall(y_pred, y)\n",
    "        self.f1(y_pred, y)\n",
    "        self.roc_auc(y_pred, y)\n",
    "        \n",
    "        self.log(\"val_step_loss\", loss)        \n",
    "        return loss\n",
    "    \n",
    "    def on_validation_epoch_end(self) -> None:\n",
    "        \"\"\"\n",
    "        Log the learning rate at the end of the validation epoch.\n",
    "        \"\"\"\n",
    "        self.log(\"val_ep_accuracy\", self.accuracy, on_epoch=True)\n",
    "        self.log(\"val_ep_precision\", self.precision, on_epoch=True)\n",
    "        self.log(\"val_ep_recall\", self.recall, on_epoch=True)\n",
    "        self.log(\"val_ep_f1\", self.f1, on_epoch=True)\n",
    "        self.log(\"val_ep_roc_auc\", self.roc_auc, on_epoch=True)\n",
    "    \n",
    "    def test_step(self, batch: torch.Tensor, batch_idx: int) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Test step of the model.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        batch : torch.Tensor\n",
    "            The input batch\n",
    "        batch_idx : int\n",
    "            The index of the batch\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            The loss\n",
    "        \"\"\"\n",
    "        x, y = batch\n",
    "        y_pred = self(x)\n",
    "        loss = self.loss_func(y_pred, y)\n",
    "        \n",
    "        self.accuracy(y_pred, y)\n",
    "        self.precision(y_pred, y)\n",
    "        self.recall(y_pred, y)\n",
    "        self.f1(y_pred, y)\n",
    "        self.roc_auc(y_pred, y)\n",
    "        \n",
    "        self.log(\"test_step_loss\", loss)\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    def on_test_epoch_end(self) -> None:\n",
    "        \"\"\"\n",
    "        Log the metrics at the end of the test epoch.\n",
    "        \"\"\"\n",
    "        self.log(\"test_ep_accuracy\", self.accuracy, on_epoch=True)\n",
    "        self.log(\"test_ep_precision\", self.precision, on_epoch=True)\n",
    "        self.log(\"test_ep_recall\", self.recall, on_epoch=True)\n",
    "        self.log(\"test_ep_f1\", self.f1, on_epoch=True)\n",
    "        self.log(\"test_ep_roc_auc\", self.roc_auc, on_epoch=True)\n",
    "\n",
    "    def configure_optimizers(self) -> torch.optim.Optimizer:\n",
    "        \"\"\"\n",
    "        Configure the optimizer for the model.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        torch.optim.Optimizer\n",
    "            The optimizer\n",
    "        \"\"\"\n",
    "        return self.optimizer(self.parameters(), lr=self.initial_learning_rate)\n",
    "    \n",
    "    def _get_conv_out_shape(self, input_size: torch.Size) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Calculate shape of the output of the convolutional layers.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        input_size : torch.Size\n",
    "            The size of the input tensor\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        torch.Size\n",
    "            The size of the output tensor\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            zeros = torch.zeros(*input_size, device=self._device)\n",
    "            z = self.hidden_layers(zeros)\n",
    "            z = torch.prod(torch.tensor(z.shape))\n",
    "        return z\n",
    "\n",
    "    "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "910878ec04f8bc57"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
